Metadata-Version: 2.4
Name: web_scraping_toolkit
Version: 0.1.0
Summary: A toolkit for web scraping with advanced features like proxy rotation, captcha solving, and cache management
Author-email: Ziyan Zhou <benz92124@gmail.com>
Project-URL: Homepage, https://github.com/benzdriver/web_scraping_toolkit
Project-URL: Bug Tracker, https://github.com/benzdriver/web_scraping_toolkit/issues
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.7
Description-Content-Type: text/markdown
Requires-Dist: requests>=2.25.0
Requires-Dist: beautifulsoup4>=4.9.0
Requires-Dist: python-dotenv>=0.15.0
Requires-Dist: 2captcha-python>=1.5.1
Requires-Dist: playwright>=1.20.0
Requires-Dist: pytrends>=4.9.0

# Web Scraping Toolkit

A Python package for robust web scraping with features for proxy rotation, IP switching, captcha solving, and cache management.

## Features

- **Smart Proxy Management**: Rotate between multiple proxies, detect failures, and avoid IP bans
- **Captcha Handling**: Integrate with services like 2Captcha to solve CAPTCHAs automatically
- **Browser Automation**: Use Playwright for advanced browser-based scraping
- **Cache Management**: Efficient caching system to avoid redundant requests
- **Error Resilience**: Multiple fallback strategies when primary scraping methods fail

## Installation

```bash
pip install web-scraping-toolkit
```

## Basic Usage

```python
from web_scraping_toolkit import ProxyManager, CacheMechanism, WebScraper

# Initialize the proxy manager
proxy_manager = ProxyManager()

# Initialize the cache system
cache = CacheMechanism("my_scraping_cache")

# Create a scraper with automatic proxy rotation
scraper = WebScraper(
    proxy_manager=proxy_manager,
    cache_mechanism=cache
)

# Fetch a web page with automatic proxy rotation and caching
response = scraper.get("https://example.com")
```

## Advanced Usage

### Proxy Rotation

```python
from web_scraping_toolkit import ProxyManager

# Create a proxy manager with custom settings
proxy_manager = ProxyManager(
    rotation_interval=300,  # Rotate every 5 minutes
    max_requests_per_ip=10  # Max 10 requests per IP before rotation
)

# Get a proxy for use with requests
proxy = proxy_manager.get_proxy()

# Mark current proxy as bad (e.g., if it gets blocked)
proxy_manager.blacklist_current_proxy(duration_minutes=30)

# Get a proxy specifically formatted for Playwright
playwright_proxy = proxy_manager.get_playwright_proxy()
```

### Captcha Solving

```python
from web_scraping_toolkit import CaptchaSolver

# Initialize with your 2Captcha API key
solver = CaptchaSolver(api_key="your_2captcha_api_key")

# Solve a reCAPTCHA
solution = solver.solve_recaptcha(
    site_key="6LcXXXXXXXXXXXXXXXXXXXXX",
    page_url="https://example.com"
)

# Use with Playwright
def handle_page_with_captcha(page):
    if solver.detect_and_solve_recaptcha(page):
        print("Captcha solved successfully!")
```

### Cache Management

```python
from web_scraping_toolkit import CacheMechanism

# Initialize cache with a specific name
cache = CacheMechanism("news_scraper_cache")

# Check if an item is in cache
if cache.is_cached("https://example.com/article-1"):
    # Get cached data
    data = cache.get_cached_data("https://example.com/article-1")
else:
    # Fetch and cache new data
    data = fetch_new_data()
    cache.cache_data("https://example.com/article-1", data)

# Mark an item as processed
cache.mark_as_processed("https://example.com/article-1", stage="content_extraction")

# Get unprocessed items
unprocessed = cache.get_unprocessed_items(stage="content_extraction")
```

## Configuration

The toolkit supports configuration via environment variables or a `.env` file:

```
# .env file example
USE_PROXY=true
PROXY_ROTATION_INTERVAL=300
SMARTPROXY_USERNAME=user
SMARTPROXY_PASSWORD=pass
SMARTPROXY_ENDPOINT=gate.smartproxy.com
SMARTPROXY_PORT=7000
TWOCAPTCHA_API_KEY=your_key
```

## License

MIT 
